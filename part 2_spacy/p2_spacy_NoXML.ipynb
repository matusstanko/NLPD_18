{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dff390f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy.training import Example\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddbf1bf",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54e12212",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"output_train.csv\")\n",
    "df_valid = pd.read_csv(\"output_valid.csv\")\n",
    "df_test  = pd.read_csv(\"output_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a941a0",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8104c367",
   "metadata": {},
   "source": [
    "## Preparing data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b90b7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_data(df, text_col, label_col): # prepares data for SpaCy training\n",
    "    texts = df[text_col].tolist()\n",
    "    labels = df[label_col].tolist()\n",
    "    cats = [{\"cats\": {\"1\": bool(l), \"0\": not bool(l)}} for l in labels]\n",
    "    return list(zip(texts, cats)), texts, labels\n",
    "\n",
    "train_data, _, _= make_data(df_train, \"statement\", \"label_binary\")\n",
    "valid_data, valid_texts, valid_labels = make_data(df_valid, \"statement\", \"label_binary\")\n",
    "test_data, test_texts, test_labels = make_data(df_test, \"statement\", \"label_binary\")\n",
    "\n",
    "# build spacy model with BOW classifier\n",
    "nlp = spacy.blank(\"en\")\n",
    "textcat = nlp.add_pipe(\n",
    "    \"textcat\",\n",
    "    last=True,\n",
    "    config={\"model\": {\n",
    "            \"@architectures\": \"spacy.TextCatBOW.v3\",\n",
    "            \"exclusive_classes\": True,\n",
    "            \"ngram_size\": 1,\n",
    "            \"no_output_layer\": False}})\n",
    "# add classification labels\n",
    "textcat.add_label(\"0\")\n",
    "textcat.add_label(\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a294fa",
   "metadata": {},
   "source": [
    "## Model Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90e7fbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 — Train Loss: 474.8942 — Val F1: 64.4275%\n",
      "Epoch 2/10 — Train Loss: 375.8721 — Val F1: 62.0582%\n",
      "Epoch 3/10 — Train Loss: 321.1152 — Val F1: 60.9201%\n",
      "Epoch 4/10 — Train Loss: 287.0336 — Val F1: 61.4345%\n",
      "Epoch 5/10 — Train Loss: 261.0785 — Val F1: 61.7184%\n",
      "Epoch 6/10 — Train Loss: 241.4476 — Val F1: 62.1000%\n",
      "Epoch 7/10 — Train Loss: 226.1418 — Val F1: 62.5061%\n",
      "Epoch 8/10 — Train Loss: 213.5697 — Val F1: 58.4764%\n",
      "Epoch 9/10 — Train Loss: 201.9112 — Val F1: 59.7927%\n",
      "Epoch 10/10 — Train Loss: 192.3624 — Val F1: 60.5317%\n",
      "Test scores: {'accuracy': '67.03%', 'f1': '60.68%', 'precision': '61.34%', 'recall': '60.02%'}\n"
     ]
    }
   ],
   "source": [
    "n_iter = 10\n",
    "train_losses = []\n",
    "val_f1s = []\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "for epoch in range(n_iter):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "    # training\n",
    "    for batch in spacy.util.minibatch(train_data, size=8):\n",
    "        examples = []\n",
    "        for text, ann in batch:\n",
    "            doc = nlp.make_doc(text)\n",
    "            examples.append(Example.from_dict(doc, ann))\n",
    "        nlp.update(examples, drop=0.2, sgd=optimizer, losses=losses)\n",
    "    train_losses.append(losses[\"textcat\"])\n",
    "\n",
    "    # validation eval\n",
    "    preds = [nlp(txt).cats for txt in valid_texts]\n",
    "    pred_bin = [int(p[\"1\"] >= 0.5) for p in preds]\n",
    "    f1 = f1_score(valid_labels, pred_bin)*100\n",
    "    val_f1s.append(f1)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{n_iter} — \"\n",
    "        f\"Train Loss: {losses['textcat']:.4f} — \"\n",
    "        f\"Val F1: {f1:.4f}%\")\n",
    "\n",
    "# test evaluation\n",
    "test_preds= [nlp(t).cats for t in test_texts]\n",
    "test_bin = [int(p[\"1\"] >= 0.5) for p in test_preds]\n",
    "scores = {\n",
    "    \"accuracy\": accuracy_score(test_labels, test_bin)*100,\n",
    "    \"f1\": f1_score(test_labels, test_bin)*100,\n",
    "    \"precision\": precision_score(test_labels, test_bin)*100,\n",
    "    \"recall\": recall_score(test_labels, test_bin)*100\n",
    "}\n",
    "print(\"Test scores:\", {k: f\"{v:.2f}%\" for k,v in scores.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e559822",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5ceae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training + Validation curve\n",
    "plt.figure()\n",
    "plt.plot(range(1, n_iter+1), train_losses, marker=\"o\", label=\"Train Loss\")\n",
    "plt.plot(range(1, n_iter+1), val_f1s,      marker=\"s\", linestyle=\"--\", label=\"Val F1 (%)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Training Loss & Validation F1 (%)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_validation_curve_noXML.png\")\n",
    "plt.close()\n",
    "\n",
    "# Confusion matrix\n",
    "cm   = confusion_matrix(test_labels, test_bin)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=[\"0\",\"1\"])\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrix_noXML.png\")\n",
    "plt.close()\n",
    "\n",
    "# ROC curve\n",
    "probs   = [p[\"1\"] for p in test_preds]\n",
    "fpr, tpr, _ = roc_curve(test_labels, probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"roc_curve_noXML.png\")\n",
    "plt.close()\n",
    "\n",
    "# Save results\n",
    "pd.DataFrame([scores]).to_csv(\"spacy_results_noXML.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
