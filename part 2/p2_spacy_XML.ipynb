{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6d502c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from spacy.util import minibatch\n",
    "from spacy.training import Example\n",
    "\n",
    "random.seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce04c346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "experiment_configs = [{\n",
    "    \"name\": \"spacy_xml_tags\",\n",
    "    \"epochs\": 7,\n",
    "    \"batch_size\": 16\n",
    "}]\n",
    "\n",
    "# load data\n",
    "df_train = pd.read_csv(\"output_train.csv\").drop(columns=['A_raw_entities']).copy() # spacy tagged only\n",
    "df_test  = pd.read_csv(\"output_test.csv\").drop(columns=['A_raw_entities']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3f2bdeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18369/18369 [00:00<00:00, 218654.70it/s]\n",
      "100%|██████████| 2296/2296 [00:00<00:00, 208674.55it/s]\n"
     ]
    }
   ],
   "source": [
    "def add_offsets(text, entities): # adds start and end pos.\n",
    "    if not isinstance(entities, list):\n",
    "        return []\n",
    "    # ------------------------------------------------\n",
    "    used = [False] * len(text) # tracks characters already tagged\n",
    "    results = []\n",
    "\n",
    "    for ent in entities: \n",
    "        word  = ent.get(\"word\")\n",
    "        label = ent.get(\"entity\", \"\").lower()\n",
    "        if not word or not label: # skip empty\n",
    "            continue\n",
    "        pattern = re.escape(word)\n",
    "        match   = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            s, e = match.start(), match.end()\n",
    "            if not any(used[s:e]):\n",
    "                results.append({\"start\": s, \"end\": e, \"entity\": label})\n",
    "                for i in range(s, e):\n",
    "                    used[i] = True \n",
    "    return results\n",
    "\n",
    "def merge_adjacent_entities(entities): # merge if same label\n",
    "    if not entities: return []\n",
    "    # ------------------------------------------------\n",
    "    entities = sorted(entities, key=lambda x: x['start']) # sort so left to right\n",
    "    merged = [entities[0]] # leftmost entity\n",
    "    for curr in entities[1:]: # the rest\n",
    "        last = merged[-1]\n",
    "        if curr['entity'] == last['entity'] and curr['start'] <= last['end'] + 1:\n",
    "                # same label & touching\n",
    "            last['end'] = curr['end']\n",
    "        else:\n",
    "            merged.append(curr)\n",
    "    return merged\n",
    "\n",
    "def insert_xml_tags(text, entities): # put entity tags into text\n",
    "    if not entities:\n",
    "        return text\n",
    "    # ------------------------------------------------\n",
    "    # helpers\n",
    "    ents = add_offsets(text, entities) # add start & end\n",
    "    merged = merge_adjacent_entities(ents) # merge adjacent\n",
    "\n",
    "    offset = 0\n",
    "    # insert <entity tags>\n",
    "    for ent in sorted(merged, key=lambda x: x['start']):\n",
    "        tag_open  = f\"<{ent['entity']}>\"\n",
    "        tag_close = f\"</{ent['entity']}>\"\n",
    "        s = ent['start'] + offset\n",
    "        e = ent['end']   + offset\n",
    "        text = text[:s] + tag_open + text[s:e] + tag_close + text[e:]\n",
    "        offset += len(tag_open) + len(tag_close)\n",
    "    return text\n",
    "\n",
    "# apply XML tagging\n",
    "def tag_statement(row):\n",
    "    return insert_xml_tags(row['statement'], row['B_raw_entities'])\n",
    "\n",
    "tqdm.pandas() # progress bar\n",
    "for df in (df_train, df_test):\n",
    "    df['B_XML_statement'] = df.progress_apply(tag_statement, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c9fbceab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 — Loss: 244.4166 — F1: 0.5307\n",
      "Epoch 2/7 — Loss: 205.6575 — F1: 0.6710\n",
      "Epoch 3/7 — Loss: 164.7764 — F1: 0.5872\n",
      "Epoch 4/7 — Loss: 123.7194 — F1: 0.6194\n",
      "Epoch 5/7 — Loss: 89.8225 — F1: 0.6075\n",
      "Epoch 6/7 — Loss: 70.6893 — F1: 0.6003\n",
      "Epoch 7/7 — Loss: 57.8541 — F1: 0.5321\n"
     ]
    }
   ],
   "source": [
    "# prepare data for textcat\n",
    "def prepare_spacy_data(df):\n",
    "    return [ # prepares data for SpaCy training\n",
    "        (text, {\"cats\": {\"TRUE\": label == 1, \"FALSE\": label == 0}})\n",
    "        for text, label in zip(df['B_XML_statement'], df['label_binary'])\n",
    "    ]\n",
    "train_data = prepare_spacy_data(df_train)\n",
    "test_data  = prepare_spacy_data(df_test)\n",
    "\n",
    "# run experiments\n",
    "all_results = []\n",
    "\n",
    "for cfg in experiment_configs:\n",
    "    nlp = spacy.blank(\"en\") # empty Eng pipeline for SpaCy\n",
    "    textcat = nlp.add_pipe(\"textcat\", last=True) # text clf\n",
    "    # register classes\n",
    "    textcat.add_label(\"TRUE\")\n",
    "    textcat.add_label(\"FALSE\")\n",
    "\n",
    "    # initialize optimizer\n",
    "    optimizer = nlp.initialize()\n",
    "\n",
    "    losses_list, f1s = [], [] # losses and f1 scores\n",
    "\n",
    "    for epoch in range(cfg['epochs']):\n",
    "        random.shuffle(train_data) # shuffle training data\n",
    "        batches = minibatch(train_data, size=cfg['batch_size']) # create mini batches\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch) # unzip into texts and annotations\n",
    "            examples = [ # for spacy\n",
    "                Example.from_dict(nlp.make_doc(txt), ann)\n",
    "                for txt, ann in zip(texts, annotations)]\n",
    "            losses = {} # feed into the model, train\n",
    "            nlp.update(examples, sgd=optimizer, losses=losses)\n",
    "            epoch_loss += losses.get(\"textcat\", 0.0)\n",
    "\n",
    "        # evaluate on test set\n",
    "        preds, true_labels = [], [] # predictions and true labels\n",
    "        for txt, ann in test_data:\n",
    "            doc = nlp(txt)\n",
    "            pred = doc.cats[\"TRUE\"] > 0.5\n",
    "            preds.append(int(pred))\n",
    "            true_labels.append(int(ann[\"cats\"][\"TRUE\"]))\n",
    "\n",
    "        f1 = f1_score(true_labels, preds)\n",
    "        losses_list.append(epoch_loss)\n",
    "        f1s.append(f1)\n",
    "        print(f\"Epoch {epoch+1}/{cfg['epochs']} — Loss: {epoch_loss:.4f} — F1: {f1:.4f}\")\n",
    "\n",
    "    # plot metrics\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, cfg['epochs']+1), losses_list, marker='o', label='Loss')\n",
    "    plt.plot(range(1, cfg['epochs']+1), [f * 100 for f in f1s], marker='s', linestyle='--', label='F1')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.title(\"SpaCy + XML Training Curve\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{cfg['name']}_curve.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # final metrics\n",
    "    acc  = accuracy_score(true_labels, preds)\n",
    "    prec = precision_score(true_labels, preds)\n",
    "    rec  = recall_score(true_labels, preds)\n",
    "\n",
    "    all_results.append({\n",
    "        \"name\": cfg[\"name\"],\n",
    "        \"epochs\": cfg[\"epochs\"],\n",
    "        \"batch_size\": cfg[\"batch_size\"],\n",
    "        \"accuracy\": round(acc, 2),\n",
    "        \"f1\": round(f1, 2),\n",
    "        \"precision\": round(prec, 2),\n",
    "        \"recall\": round(rec, 2)\n",
    "    })\n",
    "\n",
    "# save results\n",
    "pd.DataFrame(all_results).to_csv(\"spacy_with_xml_results.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
